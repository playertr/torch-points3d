{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "DIR = \"/home/tim/Research/torch-points3d/\"\n",
    "sys.path.insert(0, DIR)\n",
    "\n",
    "from torch_points3d.datasets.grasp_classification import acronymvid\n",
    "from torch_points3d.models.grasp_classification import minkowski\n",
    "from torch_points3d.metrics.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_dir = \"/home/tim/Research/torch-points3d/outputs/2021-08-04/17-03-22\"\n",
    "# ckpt_dir = \"/home/tim/Research/torch-points3d/outputs/2021-08-04/18-14-55\"\n",
    "# ckpt_dir = \"/home/tim/Research/torch-points3d/outputs/2021-08-04/23-53-23\"\n",
    "# check_name=\"MinkUNet14A\"\n",
    "\n",
    "\n",
    "# ckpt_dir = \"/home/tim/Research/torch-points3d/outputs/2021-08-06/17-28-38\"\n",
    "# ckpt_dir = \"/home/tim/Research/torch-points3d/outputs/2021-08-12/10-51-01\"\n",
    "# check_name=\"STRes16UNet14B\"\n",
    "\n",
    "# ckpt_dir = \"/home/tim/Research/torch-points3d/outputs/2021-08-13/18-53-47\"\n",
    "# check_name=\"GraspSTRes16UNet14B\"\n",
    "\n",
    "# ckpt_dir = \"/home/tim/Research/torch-points3d/outputs/2021-08-16/14-23-59\"\n",
    "# check_name=\"GraspSTRes16UNet14B\"\n",
    "\n",
    "# from the overnight cluster run\n",
    "# ckpt_dir = \"/home/tim/Research/torch-points3d/outputs/2021-08-21/01-06-13\"\n",
    "# check_name = \"GraspMinkUNet14A\"\n",
    "\n",
    "ckpt_dir = \"/home/tim/Research/torch-points3d/outputs/2021-08-21/12-51-16\"\n",
    "check_name = \"GraspMinkUNet14A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_config = \"\"\"\n",
    "data:\n",
    "    task: grasp_classification\n",
    "    class: acronymvid.AcronymVidDataset\n",
    "    name: acronymvid\n",
    "    dataroot: /home/tim/Research/GraspRefinement/data\n",
    "    process_workers: 8\n",
    "    apply_rotation: False\n",
    "    grid_size: 0.1\n",
    "    mode: \"last\"\n",
    "\n",
    "    train_pre_batch_collate_transform:\n",
    "    - transform: ClampBatchSize\n",
    "      params:\n",
    "        num_points: 1000000\n",
    "\n",
    "    train_transform:\n",
    "    - transform: Random3AxisRotation\n",
    "      params:\n",
    "        apply_rotation: ${data.apply_rotation}\n",
    "        rot_x: 8\n",
    "        rot_y: 8\n",
    "        rot_z: 180\n",
    "    - transform: RandomSymmetry\n",
    "      params:\n",
    "        axis: [True, True, False]\n",
    "    - transform: GridSampling3D\n",
    "      params:\n",
    "        size: ${data.grid_size}\n",
    "        quantize_coords: True\n",
    "        mode: ${data.mode}\n",
    "\n",
    "    val_transform:\n",
    "    - transform: GridSampling3D\n",
    "      params:\n",
    "        size: ${data.grid_size}\n",
    "        quantize_coords: True\n",
    "        mode: ${data.mode}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "params = OmegaConf.create(yaml_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = acronymvid.AcronymVidDataset(params.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(ckpt_dir, check_name, \"test\", resume=False)\n",
    "\n",
    "model = model_checkpoint.create_model(dataset, weight_name=\"latest\").to(device)\n",
    "model.eval()\n",
    "\n",
    "dataset.create_dataloaders(model,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    precompute_multi_scale=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = dataset.test_dataloaders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "import numpy as np\n",
    "def plot_point_cloud(data, out_preds, out_coords, path=\"figs/out.gif\"):\n",
    "    p = pv.Plotter(notebook=True, shape=(1,2))\n",
    "    p.open_movie(path)\n",
    "    for t in np.unique(data.time.numpy()):\n",
    "        p.clear()\n",
    "        # plot ground truth on left\n",
    "        p.subplot(0, 0)\n",
    "        point_cloud = pv.PolyData(data.pos[data.time == t].numpy())\n",
    "        point_cloud['y'] = data.y[data.time == t].numpy()\n",
    "        p.add_points(point_cloud, show_scalar_bar=False, point_size=3)\n",
    "        p.camera_position = [0, 3, -6]\n",
    "\n",
    "        # plot prediction on right\n",
    "        p.subplot(0, 1)\n",
    "        out_pos = out_coords[:,-3:]\n",
    "        out_time = out_coords[:,-4]\n",
    "        point_cloud2 = pv.PolyData(out_pos[out_time == t])\n",
    "        point_cloud2['y'] = np.array(out_preds[out_time == t] > 0, dtype=np.float)\n",
    "        p.add_points(point_cloud2, show_scalar_bar=False, point_size=3)\n",
    "\n",
    "        p.camera_position = [0, 3, -6]\n",
    "        p.render()\n",
    "        p.write_frame()\n",
    "        # p.show(jupyter_backend='static')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(tqdm(loader)):\n",
    "        # data = dataset.test_dataset[0][i]\n",
    "        model.set_input(data, device)\n",
    "        model.forward()\n",
    "\n",
    "        # Extract first example from this batch\n",
    "        example = model.data.get_example(0)\n",
    "        # out_preds = model.output.features_at(0).detach().numpy()\n",
    "        # out_coords = model.output.coordinates_at(0).detach().numpy()\n",
    "        out_coords = model.input.coordinates.detach().cpu().numpy()\n",
    "\n",
    "        out_preds = model.class_logits.detach().cpu().numpy()\n",
    "        out_preds = out_preds[out_coords[:,0] == 0]\n",
    "        out_preds = out_preds > 0\n",
    "\n",
    "        out_coords = out_coords[out_coords[:,0] == 0]\n",
    "        \n",
    "\n",
    "        name = \"figs/\" + check_name + \"\".join(ckpt_dir.split('/')[-2:]) + f\"/{i}.gif\"\n",
    "        os.makedirs(os.path.dirname(name), exist_ok=True)\n",
    "        plot_point_cloud(example, out_preds, out_coords, path=name)\n",
    "\n",
    "        \n",
    "        # out = (out.reshape(-1, 300, 300) > 0).astype('uint8') * 255\n",
    "\n",
    "        # for im in out:\n",
    "        #     plt.imshow(im)\n",
    "        #     plt.pause(0.01)\n",
    "\n",
    "        # gt = data.y.detach().numpy().reshape(-1, 300, 300)\n",
    "        # for im in gt:\n",
    "        #     plt.imshow(im)\n",
    "        #     plt.pause(0.01)\n",
    "\n",
    "        # name = check_name + \"\".join(ckpt_dir.split('/')[-2:]) + f\"_{i}.gif\"\n",
    "        # imageio.mimsave(f\"figs/preds_{name}\", out)\n",
    "\n",
    "        # gtname = f\"gt_{i}.gif\"\n",
    "        # imageio.mimsave(f\"figs/{gtname}\", data.y.detach().numpy().reshape(-1, 300, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.sigmoid(model.class_logits).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "\n",
    "ad =  model.approach_dir.detach().cpu().numpy()\n",
    "ax.quiver(0, 0, 0, ad[:,0], ad[:,1], ad[:,2], normalize=True)\n",
    "ax.set_title(\"Approach Direction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 20\n",
    "fig, axs = plt.subplots(1, 3, sharey=True, tight_layout=True)\n",
    "\n",
    "axs[0].hist(ad[:,0], bins=n_bins)\n",
    "axs[1].hist(ad[:,1], bins=n_bins)\n",
    "axs[2].hist(ad[:,2], bins=n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "import pyvista as pv\n",
    "from pyvista import examples\n",
    "pn.extension('vtk', sizing_mode=\"stretch_width\")\n",
    "import numpy as np\n",
    "\n",
    "# make point cloud from my data\n",
    "data = example\n",
    "t = 0\n",
    "pos = data.pos[data.time == t].numpy()\n",
    "point_cloud = pv.PolyData(pos, )\n",
    "point_cloud['y'] = data.y[data.time == t].numpy()\n",
    "point_cloud[\"radius\"] = np.ones(len(pos))\n",
    "\n",
    "# pyvista plotter\n",
    "pl = pv.Plotter(notebook=True)\n",
    "actor = pl.add_points(point_cloud, render_points_as_spheres=True, point_size=5)\n",
    "pl.camera_position = [-1, 5, 10] #set camera position\n",
    "\n",
    "# save initial camera properties\n",
    "renderer = list(pl.ren_win.GetRenderers())[0]\n",
    "initial_camera = renderer.GetActiveCamera()\n",
    "initial_camera_pos = {\"focalPoint\": initial_camera.GetFocalPoint(),\n",
    "                      \"position\": initial_camera.GetPosition(),\n",
    "                      \"viewUp\": initial_camera.GetViewUp()}\n",
    "\n",
    "# Panel creation using the VTK Scene created by the plotter pyvista\n",
    "orientation_widget = True\n",
    "enable_keybindings = True\n",
    "vtkpan = pn.panel(pl.ren_win, sizing_mode='stretch_both', orientation_widget=orientation_widget,\n",
    "                  enable_keybindings=enable_keybindings, height=600)\n",
    "vtkpan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvista import demos\n",
    "\n",
    "# basic glyphs demo\n",
    "mesh = demos.glyphs(2)\n",
    "\n",
    "text = demos.logo.text_3d(\"I'm interactive!\", depth=0.2)\n",
    "text.points *= 0.1\n",
    "text.translate([0, 1.4, 1.5])\n",
    "mesh += text\n",
    "mesh['Example Scalars'] = mesh.points[:, 0]\n",
    "\n",
    "mesh.plot(cpos='xy', jupyter_backend='ipygany', background='white',\n",
    "          show_scalar_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "134867c5b36f700baaa67a27b0b6dfc99f4165b58a173e2e8d72841bf053541a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('gref': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
